# Introduction

:::{admonition} Working with Large Data
:class: important
When we work with small or moderately sized datasets, we can ignore computational contraints, such as the amount of RAM available, or how long it takes to process. At some point however, size starts to matter, and we have to be more careful about creating a data workflow that actually *works*. In this workshop we will look at how to deal with larger datasets using Kellogg resources, focusing on:

- Writing efficient queries for datasets on the Kellogg Data Cloud (KDC)
- Loading and processing datasets on the Kellogg Linux Cluster (KLC)
- Monitoring resource usage on KDC and KLC
- Automating, testsing, and parallelizing workflows
:::

:::{admonition} Data Workflow
:class: note

```{figure} ./images/data-workflow-intro.png
---
width: 900px
name: data-workflow-intro
---
```
:::