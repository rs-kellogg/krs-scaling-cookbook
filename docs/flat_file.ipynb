{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling Example: Flat files\n",
    "\n",
    "We'll work with a [dataset](https://www.kaggle.com/datasets/sunnykakar/spotify-charts-all-audio-data) of Spotify Charts available on Kaggle. Here's the description according to the authors.\n",
    "\n",
    "\n",
    "> This is a complete dataset of all the \"Top 200\" and \"Viral 50\" charts published globally by Spotify. Spotify publishes a new chart every 2-3 days. This is its entire collection since January 1, 2019. This dataset is a continuation of the Kaggle Dataset: Spotify Charts but contains 29 rows for each row that was populated using the Spotify API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To download the data, you only need to create a free Kaggle account. The downloaded data is a zipped archive which contains one CSV file called \"merged_data.csv\". Before loading in any data, we can use the disk utilities via Python's os module to see the size of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File size: 25.24 GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "def bytes_to_human_readable(size):\n",
    "    units = ['bytes', 'KB', 'MB', 'GB', 'TB']\n",
    "    for i in range(len(units)):\n",
    "        if size < 1024:\n",
    "            break\n",
    "        size /= 1024\n",
    "    return size, units[i]\n",
    "\n",
    "def show_file_size(path):\n",
    "    ## get file size in bytes\n",
    "    file_size = os.path.getsize(path)\n",
    "\n",
    "    ## convert to human readable format\n",
    "    file_size, unit = bytes_to_human_readable(file_size)\n",
    "    print(f'File size: {file_size:.2f} {unit}')\n",
    "    \n",
    "def show_dataframe_memory_usage(df,deep=False):\n",
    "    ## get memory usage in bytes\n",
    "    mem_usage = df.memory_usage(deep=deep).sum()\n",
    "\n",
    "    ## convert to human readable format\n",
    "    mem_usage, unit = bytes_to_human_readable(mem_usage)\n",
    "    print(f'DataFrame memory usage: {mem_usage:.2f} {unit}')\n",
    "    \n",
    "show_file_size('merged_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use the \"ls -lh\" command to list the files and their size. The -l tag puts the files into a stacked list and -h gives the size in \"human\" units (KB, MB, GB, etc., instead of bytes.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 jat4714 jat4714 26G Jul  6 14:51 merged_data.csv\n"
     ]
    }
   ],
   "source": [
    "ls -lh merged_data.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note on compressed data\n",
    "\n",
    "While this tutorial is mostly about conserving memory, now might also be a good time to talk about conserving hard disk space as well. You can compress CSV files and still load in the dataset into Python without needing to decompress them. I used gzip to compress the CSV file in this example and you can see it takes up way less space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File size: 2.83 GB\n"
     ]
    }
   ],
   "source": [
    "show_file_size('merged_data.csv.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, while we've managed to compress the data in its archived state, we still need to worry about the size of the uncompressed data. Unfortunately, the compression ratio is can vary a lot depending on the type of data and method of compression, so it is not trivial to calculate the size of the decompressed data before decompression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are working on a laptop, you most likely have 2-8 GB of RAM. Therefore, we cannot load the entire dataset into memory. So, what do we do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data exploration\n",
    "\n",
    "We can read a preview the compressed CSV file directly into Pandas using a couple of additional keyword parameters.\n",
    "\n",
    "- nrows: number of rows (beyond the header) that you want to load\n",
    "- compression: the name of the algorithm used to compress the file. Pandas is usually smart enough to figure this out based on the file extension, but it's safer to provide this directly. A gzipped file will usually have the extension '.gz', so we'll put 'gzip' here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "df = pd.read_csv('merged_data.csv.gz', \n",
    "                 compression='gzip',\n",
    "                 nrows=10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame memory usage: 2.15 MB\n"
     ]
    }
   ],
   "source": [
    "## df.info() will also show memory usage\n",
    "show_dataframe_memory_usage(df,deep=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame memory usage: 13.40 MB\n"
     ]
    }
   ],
   "source": [
    "## df.info(memory_usage='deep') also displays this info\n",
    "show_dataframe_memory_usage(df,deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data itself takes up 2.1 MB. However, the entire object takes up 13.4 MB. This is because of all of the indexing and metadata stored in the object itself. Therefore, it's a good rule of thumb multiply the file size by 10 when budgeting memory usage. This effect is true of both R and Stata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column selection and data type assignment\n",
    "\n",
    "You can save space if you only select the columns that are relevant for your work. It can also be taxing on the memory usage for the function to automatically assign data types to each column. You can specify the data types ahead of time with a dictionary and using Pandas data types.\n",
    "\n",
    "Let's say we want to track the performance of a specific song in a specific region on the Spotify Charts. We only need to track the rank and date after filtering on region, artist, and song."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = {\n",
    "    'title': pd.StringDtype(),\n",
    "    'rank': pd.Int32Dtype(),\n",
    "    'date': pd.StringDtype(),\n",
    "    'artist': pd.StringDtype(),\n",
    "    'region': pd.StringDtype(),\n",
    "    'chart': pd.StringDtype(),\n",
    "}\n",
    "\n",
    "df = pd.read_csv('merged_data.csv.gz', \n",
    "                 compression='gzip',\n",
    "                 nrows=10_000,\n",
    "                 usecols=dtypes.keys(),\n",
    "                 dtype=dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breaking the analysis up into chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to process the remaining dataset. We can use the chunksize parameter to only pull out a few thousand rows at a time for processing before we continue. This keyword parameter causes the function to output a generator object, which is an iterable object but only one value is kept in memory at a time.\n",
    "\n",
    "Below is an example of really simple generator. This function is the equivalent of the range function, but only pulls out one value at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a function that will loop through each chunk of the data, filter on the song/artist and region, and append data to an existing frame. We also want to keep the memory footprint of the filtered data, so we'll put a cap on the total number of rows we can keep in memory and periodically dump the results to hard disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data(title=None, artist=None, region=None):\n",
    "    dtypes = {\n",
    "        'title': pd.StringDtype(),\n",
    "        'rank': pd.Int32Dtype(),\n",
    "        'date': pd.StringDtype(),\n",
    "        'artist': pd.StringDtype(),\n",
    "        'region': pd.StringDtype(),\n",
    "        'chart': pd.StringDtype()\n",
    "    }\n",
    "\n",
    "    filtered_data = pd.DataFrame()\n",
    "    i = 0\n",
    "    \n",
    "    ## read in only 10k rows at a time and only the columns we care about\n",
    "    for df in pd.read_csv('merged_data.csv.gz', \n",
    "                    compression='gzip',\n",
    "                    chunksize=10_000,\n",
    "                    usecols=dtypes.keys(),\n",
    "                    dtype=dtypes):\n",
    "        \n",
    "        # select only the top200 chart\n",
    "        df = df[df['chart'].eq('top200')]\n",
    "            \n",
    "        ## filter by title, artist, region\n",
    "        if title is not None:\n",
    "            df = df[df['title'].str.contains(title, case=False)]\n",
    "        if artist is not None:\n",
    "            df = df[df['artist'].str.contains(artist, case=False)]\n",
    "        if region is not None:\n",
    "            df = df[df['region'].str.contains(region, case=False)]\n",
    "        \n",
    "        ## \"Smart\" way (i.e. memory-saving) to append data!\n",
    "        ## append to filtered_data if the total number of rows is less than 10_000\n",
    "        ## otherwise dump data and create a new frame\n",
    "        if len(filtered_data) + len(df) <= 10_000:\n",
    "            filtered_data = pd.concat([filtered_data, df])\n",
    "        else:\n",
    "            filtered_data.to_csv(f'filtered_data_{i}.csv', index=False)\n",
    "            i += 1\n",
    "            filtered_data = df\n",
    "            \n",
    "        ## save the last chunk\n",
    "        filtered_data.to_csv(f'filtered_data_{i}.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pull out all Taylor Swift songs on the top 200 chart in the US."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## let's pull out all Taylor Swift songs in the United States\n",
    "filter_data(artist='taylor swift', \n",
    "            region='united states')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "len(glob.glob('filtered_data_*.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output generated only one file (<10k records), so we are safe to read all the filtered data into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([pd.read_csv(f,dtype=dtypes) for f in glob.glob('filtered_data*.csv')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>rank</th>\n",
       "      <th>date</th>\n",
       "      <th>artist</th>\n",
       "      <th>region</th>\n",
       "      <th>chart</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I Don’t Wanna Live Forever (Fifty Shades Darke...</td>\n",
       "      <td>15</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>ZAYN, Taylor Swift</td>\n",
       "      <td>United States</td>\n",
       "      <td>top200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>End Game</td>\n",
       "      <td>193</td>\n",
       "      <td>2018-03-01</td>\n",
       "      <td>Taylor Swift, Ed Sheeran, Future</td>\n",
       "      <td>United States</td>\n",
       "      <td>top200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I Don’t Wanna Live Forever (Fifty Shades Darke...</td>\n",
       "      <td>9</td>\n",
       "      <td>2017-01-02</td>\n",
       "      <td>ZAYN, Taylor Swift</td>\n",
       "      <td>United States</td>\n",
       "      <td>top200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I Don’t Wanna Live Forever (Fifty Shades Darke...</td>\n",
       "      <td>3</td>\n",
       "      <td>2017-02-01</td>\n",
       "      <td>ZAYN, Taylor Swift</td>\n",
       "      <td>United States</td>\n",
       "      <td>top200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I Don’t Wanna Live Forever (Fifty Shades Darke...</td>\n",
       "      <td>6</td>\n",
       "      <td>2017-01-03</td>\n",
       "      <td>ZAYN, Taylor Swift</td>\n",
       "      <td>United States</td>\n",
       "      <td>top200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  rank        date  \\\n",
       "0  I Don’t Wanna Live Forever (Fifty Shades Darke...    15  2017-01-01   \n",
       "1                                           End Game   193  2018-03-01   \n",
       "2  I Don’t Wanna Live Forever (Fifty Shades Darke...     9  2017-01-02   \n",
       "3  I Don’t Wanna Live Forever (Fifty Shades Darke...     3  2017-02-01   \n",
       "4  I Don’t Wanna Live Forever (Fifty Shades Darke...     6  2017-01-03   \n",
       "\n",
       "                             artist         region   chart  \n",
       "0                ZAYN, Taylor Swift  United States  top200  \n",
       "1  Taylor Swift, Ed Sheeran, Future  United States  top200  \n",
       "2                ZAYN, Taylor Swift  United States  top200  \n",
       "3                ZAYN, Taylor Swift  United States  top200  \n",
       "4                ZAYN, Taylor Swift  United States  top200  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
