{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling Example: Flat files\n",
    "\n",
    "We'll work with a [dataset](https://www.kaggle.com/datasets/sunnykakar/spotify-charts-all-audio-data) of Spotify Charts available on Kaggle. Here's the description according to the authors.\n",
    "\n",
    "\n",
    "> This is a complete dataset of all the \"Top 200\" and \"Viral 50\" charts published globally by Spotify. Spotify publishes a new chart every 2-3 days. This is its entire collection since January 1, 2019. This dataset is a continuation of the Kaggle Dataset: Spotify Charts but contains 29 rows for each row that was populated using the Spotify API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To download the data, you only need to create a free Kaggle account. The downloaded data is a zipped archive which contains one CSV file called \"merged_data.csv\". Before loading in any data, we can use the disk utilities via Python's os module to see the size of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File size: 25.24 GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def show_file_size(path):\n",
    "    ## get file size in bytes\n",
    "    file_size = os.path.getsize(path)\n",
    "\n",
    "    ## convert to human readable format\n",
    "    units = ['bytes', 'KB', 'MB', 'GB', 'TB']\n",
    "    for i in range(len(units)):\n",
    "        if file_size < 1024:\n",
    "            break\n",
    "        file_size /= 1024\n",
    "    print(f'File size: {file_size:.2f} {units[i]}')\n",
    "    \n",
    "show_file_size('merged_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use the \"ls -lh\" command to list the files and their size. The -l tag puts the files into a stacked list and -h gives the size in \"human\" units (KB, MB, GB, etc., instead of bytes.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--@ 1 tracijohnson  staff    25G Apr 15 20:43 merged_data.csv\n"
     ]
    }
   ],
   "source": [
    "ls -lh merged_data.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note on compressed data\n",
    "\n",
    "While this tutorial is mostly about conserving memory, now might also be a good time to talk about conserving hard disk space as well. You can compress CSV files and still load in the dataset into Python without needing to decompress them. I used gzip to compress the CSV file in this example and you can see it takes up way less space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File size: 2.81 GB\n"
     ]
    }
   ],
   "source": [
    "show_file_size('merged_data.csv.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, while we've managed to compress the data in its archived state, we still need to worry about the size of the uncompressed data. Unfortunately, the compression ratio is can vary a lot depending on the type of data and method of compression, so it is not trivial to calculate the size of the decompressed data before decompression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are working on a laptop, you most likely have 2-8 GB of RAM. Therefore, we cannot load the entire dataset into memory. So, what do we do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data exploration\n",
    "\n",
    "We can read a preview the compressed CSV file directly into Pandas using a couple of additional keyword parameters.\n",
    "\n",
    "- nrows: number of rows (beyond the header) that you want to load\n",
    "- compression: the name of the algorithm used to compress the file. Pandas is usually smart enough to figure this out based on the file extension, but it's safer to provide this directly. A gzipped file will usually have the extension '.gz', so we'll put 'gzip' here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('merged_data.csv.gz', \n",
    "                 compression='gzip',\n",
    "                 nrows=10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 29 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   Unnamed: 0           10000 non-null  int64  \n",
      " 1   title                10000 non-null  object \n",
      " 2   rank                 10000 non-null  int64  \n",
      " 3   date                 10000 non-null  object \n",
      " 4   artist               10000 non-null  object \n",
      " 5   url                  10000 non-null  object \n",
      " 6   region               10000 non-null  object \n",
      " 7   chart                10000 non-null  object \n",
      " 8   trend                10000 non-null  object \n",
      " 9   streams              10000 non-null  float64\n",
      " 10  track_id             10000 non-null  object \n",
      " 11  album                9998 non-null   object \n",
      " 12  popularity           10000 non-null  float64\n",
      " 13  duration_ms          10000 non-null  float64\n",
      " 14  explicit             10000 non-null  bool   \n",
      " 15  release_date         10000 non-null  object \n",
      " 16  available_markets    10000 non-null  object \n",
      " 17  af_danceability      10000 non-null  float64\n",
      " 18  af_energy            10000 non-null  float64\n",
      " 19  af_key               10000 non-null  float64\n",
      " 20  af_loudness          10000 non-null  float64\n",
      " 21  af_mode              10000 non-null  float64\n",
      " 22  af_speechiness       10000 non-null  float64\n",
      " 23  af_acousticness      10000 non-null  float64\n",
      " 24  af_instrumentalness  10000 non-null  float64\n",
      " 25  af_liveness          10000 non-null  float64\n",
      " 26  af_valence           10000 non-null  float64\n",
      " 27  af_tempo             10000 non-null  float64\n",
      " 28  af_time_signature    10000 non-null  float64\n",
      "dtypes: bool(1), float64(15), int64(2), object(11)\n",
      "memory usage: 2.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.145893096923828"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.memory_usage().sum() / 1024**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
