{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling Example: Flat files\n",
    "\n",
    "We'll work with a [dataset](https://www.kaggle.com/datasets/sunnykakar/spotify-charts-all-audio-data) of Spotify Charts available on Kaggle. Here's the description according to the authors.\n",
    "\n",
    "\n",
    "> This is a complete dataset of all the \"Top 200\" and \"Viral 50\" charts published globally by Spotify. Spotify publishes a new chart every 2-3 days. This is its entire collection since January 1, 2019. This dataset is a continuation of the Kaggle Dataset: Spotify Charts but contains 29 rows for each row that was populated using the Spotify API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To download the data, you only need to create a free Kaggle account. The downloaded data is a zipped archive which contains one CSV file called \"merged_data.csv\". Before loading in any data, we can use the disk utilities via Python's os module to see the size of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File size: 25.24 GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def show_file_size(path):\n",
    "    ## get file size in bytes\n",
    "    file_size = os.path.getsize(path)\n",
    "\n",
    "    ## convert to human readable format\n",
    "    units = ['bytes', 'KB', 'MB', 'GB', 'TB']\n",
    "    for i in range(len(units)):\n",
    "        if file_size < 1024:\n",
    "            break\n",
    "        file_size /= 1024\n",
    "    print(f'File size: {file_size:.2f} {units[i]}')\n",
    "    \n",
    "show_file_size('merged_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use the \"ls -lh\" command to list the files and their size. The -l tag puts the files into a stacked list and -h gives the size in \"human\" units (KB, MB, GB, etc., instead of bytes.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--@ 1 tracijohnson  staff    25G Apr 15 20:43 merged_data.csv\n"
     ]
    }
   ],
   "source": [
    "ls -lh merged_data.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note on compressed data\n",
    "\n",
    "While this tutorial is mostly about conserving memory, now might also be a good time to talk about conserving hard disk space as well. You can compress CSV files and still load in the dataset into Python without needing to decompress them. I used gzip to compress the CSV file in this example and you can see it takes up way less space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File size: 2.81 GB\n"
     ]
    }
   ],
   "source": [
    "show_file_size('merged_data.csv.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, while we've managed to compress the data in its archived state, we still need to worry about the size of the uncompressed data. Unfortunately, the compression ratio is can vary a lot depending on the type of data and method of compression, so it is not trivial to calculate the size of the decompressed data before decompression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are working on a laptop, you most likely have 2-8 GB of RAM. Therefore, we cannot load the entire dataset into memory. So, what do we do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data exploration\n",
    "\n",
    "We can read a preview the compressed CSV file directly into Pandas using a couple of additional keyword parameters.\n",
    "\n",
    "- nrows: number of rows (beyond the header) that you want to load\n",
    "- compression: the name of the algorithm used to compress the file. Pandas is usually smart enough to figure this out based on the file extension, but it's safer to provide this directly. A gzipped file will usually have the extension '.gz', so we'll put 'gzip' here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "df = pd.read_csv('merged_data.csv.gz', \n",
    "                 compression='gzip',\n",
    "                 nrows=10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 29 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   Unnamed: 0           10000 non-null  int64  \n",
      " 1   title                10000 non-null  object \n",
      " 2   rank                 10000 non-null  int64  \n",
      " 3   date                 10000 non-null  object \n",
      " 4   artist               10000 non-null  object \n",
      " 5   url                  10000 non-null  object \n",
      " 6   region               10000 non-null  object \n",
      " 7   chart                10000 non-null  object \n",
      " 8   trend                10000 non-null  object \n",
      " 9   streams              10000 non-null  float64\n",
      " 10  track_id             10000 non-null  object \n",
      " 11  album                9998 non-null   object \n",
      " 12  popularity           10000 non-null  float64\n",
      " 13  duration_ms          10000 non-null  float64\n",
      " 14  explicit             10000 non-null  bool   \n",
      " 15  release_date         10000 non-null  object \n",
      " 16  available_markets    10000 non-null  object \n",
      " 17  af_danceability      10000 non-null  float64\n",
      " 18  af_energy            10000 non-null  float64\n",
      " 19  af_key               10000 non-null  float64\n",
      " 20  af_loudness          10000 non-null  float64\n",
      " 21  af_mode              10000 non-null  float64\n",
      " 22  af_speechiness       10000 non-null  float64\n",
      " 23  af_acousticness      10000 non-null  float64\n",
      " 24  af_instrumentalness  10000 non-null  float64\n",
      " 25  af_liveness          10000 non-null  float64\n",
      " 26  af_valence           10000 non-null  float64\n",
      " 27  af_tempo             10000 non-null  float64\n",
      " 28  af_time_signature    10000 non-null  float64\n",
      "dtypes: bool(1), float64(15), int64(2), object(11)\n",
      "memory usage: 2.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.145893096923828"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.memory_usage().sum() / 1024**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first 10k rows takes up 2.1 GB might even be able to read in a bit more. But this should be enough to get a feel for the data types involved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column selection and data type assignment\n",
    "\n",
    "You can save space if you only select the columns that are relevant for your work. It can also be taxing on the memory usage for the function to automatically assign data types to each column. You can specify the data types ahead of time with a dictionary and using Pandas data types.\n",
    "\n",
    "Let's say we want to track the performance of a specific song in a specific region on the Spotify Charts. We only need to track the rank and date after filtering on region, artist, and song."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = {\n",
    "    'title': pd.StringDtype(),\n",
    "    'rank': pd.Int32Dtype(),\n",
    "    'date': pd.StringDtype(),\n",
    "    'artist': pd.StringDtype(),\n",
    "    'region': pd.StringDtype(),\n",
    "    'chart': pd.StringDtype(),\n",
    "}\n",
    "\n",
    "df = pd.read_csv('merged_data.csv.gz', \n",
    "                 compression='gzip',\n",
    "                 nrows=10_000,\n",
    "                 usecols=dtypes.keys(),\n",
    "                 dtype=dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breaking the analysis up into chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to process the remaining dataset. We can use the chunksize parameter to only pull out a few thousand rows at a time for processing before we continue. This keyword parameter causes the function to output a generator object, which is an iterable object but only one value is kept in memory at a time.\n",
    "\n",
    "Below is an example of really simple generator. This function is the equivalent of the range function, but only pulls out one value at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def range_generator(n):\n",
    "    for i in range(n):\n",
    "        yield i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = range_generator(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object range_generator at 0x137f20e10>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "for i in g:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a function that will loop through each chunk of the data, filter on the song/artist and region, and append data to an existing frame. We also want to keep the memory footprint of the filtered data, so we'll put a cap on the total number of rows we can keep in memory and periodically dump the results to hard disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data(title=None, artist=None, region=None):\n",
    "    dtypes = {\n",
    "        'title': pd.StringDtype(),\n",
    "        'rank': pd.Int32Dtype(),\n",
    "        'date': pd.StringDtype(),\n",
    "        'artist': pd.StringDtype(),\n",
    "        'region': pd.StringDtype(),\n",
    "        'chart': pd.StringDtype()\n",
    "    }\n",
    "\n",
    "    filtered_data = pd.DataFrame()\n",
    "    i = 0\n",
    "    \n",
    "    for df in pd.read_csv('merged_data.csv.gz', \n",
    "                    compression='gzip',\n",
    "                    chunksize=10_000,\n",
    "                    usecols=dtypes.keys(),\n",
    "                    dtype=dtypes):\n",
    "        \n",
    "        # select only the top200 chart\n",
    "        df = df[df['chart'].eq('top200')]\n",
    "            \n",
    "        ## filter by title, artist, region\n",
    "        if title:\n",
    "            df = df[df['title'].str.contains(title, case=False)]\n",
    "        if artist:\n",
    "            df = df[df['artist'].str.contains(artist, case=False)]\n",
    "        if region:\n",
    "            df = df[df['region'].str.contains(region, case=False)]\n",
    "        \n",
    "        ## append to filtered_data if the total number of rows is less than 10_000\n",
    "        ## otherwise dump data and create a new frame\n",
    "        if len(filtered_data) + len(df) <= 10_000:\n",
    "            filtered_data = pd.concat([filtered_data, df])\n",
    "        else:\n",
    "            filtered_data.to_csv(f'filtered_data_{i}.csv', index=False)\n",
    "            i += 1\n",
    "            filtered_data = df\n",
    "            \n",
    "        ## save the last chunk\n",
    "        filtered_data.to_csv(f'filtered_data_{i}.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_data(title='bad blood', \n",
    "            artist='taylor swift', \n",
    "            region='united states')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "files = glob.glob('filtered_data_*.csv')\n",
    "\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turns out we only had one file generated, so it should be safe to load all of it into one frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "df = pd.concat([pd.read_csv(f,dtype=dtypes) for f in glob.glob('filtered_data*.csv')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>rank</th>\n",
       "      <th>date</th>\n",
       "      <th>artist</th>\n",
       "      <th>region</th>\n",
       "      <th>chart</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bad Blood</td>\n",
       "      <td>127</td>\n",
       "      <td>2017-06-09</td>\n",
       "      <td>Taylor Swift</td>\n",
       "      <td>United States</td>\n",
       "      <td>top200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bad Blood</td>\n",
       "      <td>122</td>\n",
       "      <td>2017-06-10</td>\n",
       "      <td>Taylor Swift</td>\n",
       "      <td>United States</td>\n",
       "      <td>top200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bad Blood</td>\n",
       "      <td>175</td>\n",
       "      <td>2017-06-11</td>\n",
       "      <td>Taylor Swift</td>\n",
       "      <td>United States</td>\n",
       "      <td>top200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bad Blood</td>\n",
       "      <td>157</td>\n",
       "      <td>2017-06-12</td>\n",
       "      <td>Taylor Swift</td>\n",
       "      <td>United States</td>\n",
       "      <td>top200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bad Blood</td>\n",
       "      <td>181</td>\n",
       "      <td>2017-06-13</td>\n",
       "      <td>Taylor Swift</td>\n",
       "      <td>United States</td>\n",
       "      <td>top200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       title  rank        date        artist         region   chart\n",
       "0  Bad Blood   127  2017-06-09  Taylor Swift  United States  top200\n",
       "1  Bad Blood   122  2017-06-10  Taylor Swift  United States  top200\n",
       "2  Bad Blood   175  2017-06-11  Taylor Swift  United States  top200\n",
       "3  Bad Blood   157  2017-06-12  Taylor Swift  United States  top200\n",
       "4  Bad Blood   181  2017-06-13  Taylor Swift  United States  top200"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This process could be very tedious if you needed to do many times for different artists. It would be wise to instead convert this file into one that can be queried using SQL. We can use duckdb to do this for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
